{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046926,
     "end_time": "2021-08-02T07:15:25.708733",
     "exception": false,
     "start_time": "2021-08-02T07:15:25.661807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "https://uku28motab.feishu.cn/docs/doccnUDbEhudHm2V440lcY87B1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036784,
     "end_time": "2021-08-02T07:15:25.784271",
     "exception": false,
     "start_time": "2021-08-02T07:15:25.747487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 1\n",
    "\n",
    "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:25.868475Z",
     "iopub.status.busy": "2021-08-02T07:15:25.867014Z",
     "iopub.status.idle": "2021-08-02T07:15:28.896609Z",
     "shell.execute_reply": "2021-08-02T07:15:28.895509Z",
     "shell.execute_reply.started": "2021-08-02T06:11:46.388737Z"
    },
    "papermill": {
     "duration": 3.072246,
     "end_time": "2021-08-02T07:15:28.896800",
     "exception": false,
     "start_time": "2021-08-02T07:15:25.824554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入相关库文件\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoConfig\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:28.990852Z",
     "iopub.status.busy": "2021-08-02T07:15:28.989719Z",
     "iopub.status.idle": "2021-08-02T07:15:28.994465Z",
     "shell.execute_reply": "2021-08-02T07:15:28.994895Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.104143Z"
    },
    "papermill": {
     "duration": 0.074658,
     "end_time": "2021-08-02T07:15:28.995048",
     "exception": false,
     "start_time": "2021-08-02T07:15:28.920390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数配置\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN = 248\n",
    "EVAL_SCHEDULE = [(0.5, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1, 1)]\n",
    "ROBERTA_PATH = \"/kaggle/input/roberta-base\"\n",
    "TOKENIZER_PATH = \"/kaggle/input/roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:29.045939Z",
     "iopub.status.busy": "2021-08-02T07:15:29.045334Z",
     "iopub.status.idle": "2021-08-02T07:15:29.061436Z",
     "shell.execute_reply": "2021-08-02T07:15:29.060994Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.208576Z"
    },
    "papermill": {
     "duration": 0.041556,
     "end_time": "2021-08-02T07:15:29.061548",
     "exception": false,
     "start_time": "2021-08-02T07:15:29.019992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "test_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "submission_df = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:29.111759Z",
     "iopub.status.busy": "2021-08-02T07:15:29.111136Z",
     "iopub.status.idle": "2021-08-02T07:15:29.416997Z",
     "shell.execute_reply": "2021-08-02T07:15:29.418181Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.240223Z"
    },
    "papermill": {
     "duration": 0.334862,
     "end_time": "2021-08-02T07:15:29.418400",
     "exception": false,
     "start_time": "2021-08-02T07:15:29.083538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 读取分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:29.512873Z",
     "iopub.status.busy": "2021-08-02T07:15:29.512091Z",
     "iopub.status.idle": "2021-08-02T07:15:29.519038Z",
     "shell.execute_reply": "2021-08-02T07:15:29.518437Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.476258Z"
    },
    "papermill": {
     "duration": 0.054538,
     "end_time": "2021-08-02T07:15:29.519206",
     "exception": false,
     "start_time": "2021-08-02T07:15:29.464668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "class LitDataset(Dataset):\n",
    "    def __init__(self, df, inference_only=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.df = df        \n",
    "        self.inference_only = inference_only\n",
    "        self.text = df.excerpt.tolist()\n",
    "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
    "        \n",
    "        if not self.inference_only:\n",
    "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
    "    \n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.text,\n",
    "            padding = 'max_length',            \n",
    "            max_length = MAX_LEN,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )        \n",
    " \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
    "        \n",
    "        if self.inference_only:\n",
    "            return (input_ids, attention_mask)            \n",
    "        else:\n",
    "            target = self.target[index]\n",
    "            return (input_ids, attention_mask, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:29.604587Z",
     "iopub.status.busy": "2021-08-02T07:15:29.603717Z",
     "iopub.status.idle": "2021-08-02T07:15:29.610534Z",
     "shell.execute_reply": "2021-08-02T07:15:29.611215Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.492515Z"
    },
    "papermill": {
     "duration": 0.053098,
     "end_time": "2021-08-02T07:15:29.611416",
     "exception": false,
     "start_time": "2021-08-02T07:15:29.558318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型\n",
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.5,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        roberta_output = self.roberta(input_ids=input_ids,\n",
    "                                      attention_mask=attention_mask)        \n",
    "\n",
    "        # There are a total of 13 layers of hidden states.\n",
    "        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
    "        # We take the hidden states from the last Roberta layer.\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        # 单元格数为 MAX_LEN.\n",
    "        # 每个单元(cell)隐藏层的大小为 768 (for roberta-base).\n",
    "\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        # 权重的形状为 BATCH_SIZE x MAX_LEN x 1\n",
    "        # 最后一层隐藏层的形状为 BATCH_SIZE x MAX_LEN x 768        \n",
    "\n",
    "        # 上下文向量的形状为 BATCH_SIZE x 768\n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        # 将上下文简化成预测分数\n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:29.695880Z",
     "iopub.status.busy": "2021-08-02T07:15:29.695124Z",
     "iopub.status.idle": "2021-08-02T07:15:29.699460Z",
     "shell.execute_reply": "2021-08-02T07:15:29.700032Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.504266Z"
    },
    "papermill": {
     "duration": 0.053211,
     "end_time": "2021-08-02T07:15:29.700234",
     "exception": false,
     "start_time": "2021-08-02T07:15:29.647023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型1 预测函数\n",
    "def predict(model, data_loader):\n",
    "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    result = np.zeros(len(data_loader.dataset))    \n",
    "    index = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "            pred = model(input_ids, attention_mask)                        \n",
    "\n",
    "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
    "            index += pred.shape[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:15:29.770921Z",
     "iopub.status.busy": "2021-08-02T07:15:29.769953Z",
     "iopub.status.idle": "2021-08-02T07:16:24.643064Z",
     "shell.execute_reply": "2021-08-02T07:16:24.644553Z",
     "shell.execute_reply.started": "2021-08-02T06:11:49.518175Z"
    },
    "papermill": {
     "duration": 54.910593,
     "end_time": "2021-08-02T07:16:24.644807",
     "exception": false,
     "start_time": "2021-08-02T07:15:29.734214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:20<01:21, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:29<00:41, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:38<00:22, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:46<00:10, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using ../input/commonlit-roberta-0467/model_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:54<00:00, 10.97s/it]\n"
     ]
    }
   ],
   "source": [
    "NUM_MODELS = 5\n",
    "\n",
    "all_predictions = np.zeros((NUM_MODELS, len(test_df)))\n",
    "\n",
    "test_dataset = LitDataset(test_df, inference_only=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                         drop_last=False, shuffle=False, num_workers=2)\n",
    "\n",
    "for model_index in tqdm(range(NUM_MODELS)):            \n",
    "    model_path = f\"../input/commonlit-roberta-0467/model_{model_index + 1}.pth\"\n",
    "    print(f\"\\nUsing {model_path}\")\n",
    "                        \n",
    "    model = LitModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n",
    "    model.to(DEVICE)\n",
    "        \n",
    "    all_predictions[model_index] = predict(model, test_loader)\n",
    "            \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:16:24.716120Z",
     "iopub.status.busy": "2021-08-02T07:16:24.715367Z",
     "iopub.status.idle": "2021-08-02T07:16:24.717711Z",
     "shell.execute_reply": "2021-08-02T07:16:24.718086Z",
     "shell.execute_reply.started": "2021-08-02T06:12:47.074141Z"
    },
    "papermill": {
     "duration": 0.031326,
     "end_time": "2021-08-02T07:16:24.718231",
     "exception": false,
     "start_time": "2021-08-02T07:16:24.686905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model1_predictions = all_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024121,
     "end_time": "2021-08-02T07:16:24.766633",
     "exception": false,
     "start_time": "2021-08-02T07:16:24.742512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 2\n",
    "Inspired from [https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3](https://www.kaggle.com/rhtsingh/commonlit-readability-prize-roberta-torch-infer-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:16:24.852344Z",
     "iopub.status.busy": "2021-08-02T07:16:24.851693Z",
     "iopub.status.idle": "2021-08-02T07:16:29.196256Z",
     "shell.execute_reply": "2021-08-02T07:16:29.197000Z",
     "shell.execute_reply.started": "2021-08-02T06:12:47.080221Z"
    },
    "papermill": {
     "duration": 4.406319,
     "end_time": "2021-08-02T07:16:29.197207",
     "exception": false,
     "start_time": "2021-08-02T07:16:24.790888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_df\n",
    "# 导入相关库文件\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import RobertaConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaModel\n",
    "from IPython.display import clear_output\n",
    "# 提取样本中的特征\n",
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent\n",
    "# 数据集寻回\n",
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        self.excerpts = self.data.excerpt.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if not self.is_test:\n",
    "            excerpt, label = self.excerpts[item], self.targets[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "                'label':torch.tensor(label, dtype=torch.double),\n",
    "            }\n",
    "        else:\n",
    "            excerpt = self.excerpts[item]\n",
    "            features = convert_examples_to_features(\n",
    "                excerpt, self.tokenizer, \n",
    "                self.max_len, self.is_test\n",
    "            )\n",
    "            return {\n",
    "                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            }\n",
    "# 模型\n",
    "class CommonLitModel(nn.Module):\n",
    "    # 初始化变量\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=True,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        \n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "            \n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    "    # 定义初始化权重\n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "                \n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    # 前向传播\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    "         # max-avg head 最大平均多头(并联了max pool和 mean pool的结果)\n",
    "        # average_pool = torch.mean(sequence_output, 1)\n",
    "        # max_pool, _ = torch.max(sequence_output, 1)\n",
    "        # concat_sequence_output = torch.cat((average_pool, max_pool), 1)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # 计算损失值\n",
    "        loss = None\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "# 读取预训练模型参数，返回预训练模型参数和分词器\n",
    "def make_model(model_name, num_labels=1):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    config = RobertaConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "# 读取处理好的文本，通过DataLoader转换成张量传入模型中\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "):\n",
    "    \n",
    "    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=test_sampler, \n",
    "        pin_memory=False, \n",
    "        drop_last=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    return test_loader\n",
    "# 测试模型\n",
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def evaluate(self, data_loader, tokenizer):\n",
    "        preds = []\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids']\n",
    "                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids\n",
    "                    )\n",
    "                \n",
    "                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n",
    "                preds += logits\n",
    "        return preds\n",
    "# 参数配置\n",
    "def config(fold, model_name, load_model_path):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    \n",
    "    max_len = 250\n",
    "    batch_size = 8\n",
    "\n",
    "    model, tokenizer = make_model(\n",
    "        model_name=model_name, \n",
    "        num_labels=1\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(\n",
    "        torch.load(f'{load_model_path}/model{fold}.bin')\n",
    "    )\n",
    "    \n",
    "    test_loader = make_loader(\n",
    "        test, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        test_loader, scaler\n",
    "    )\n",
    "# 运行程序，记录测试集训练时间并得到测试结果\n",
    "def run(fold=0, model_name=None, load_model_path=None):\n",
    "    # 读取参数\n",
    "    model, tokenizer, \\\n",
    "        test_loader, scaler = config(fold, model_name, load_model_path)\n",
    "    # 训练时间计算\n",
    "    import time\n",
    "\n",
    "    evaluator = Evaluator(model, scaler)\n",
    "    # 初始化训练时间列表\n",
    "    test_time_list = []\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic1 = time.time()\n",
    "\n",
    "    preds = evaluator.evaluate(test_loader, tokenizer)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    tic2 = time.time() \n",
    "    test_time_list.append(tic2 - tic1)\n",
    "    \n",
    "    del model, tokenizer, test_loader, scaler\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:16:29.253667Z",
     "iopub.status.busy": "2021-08-02T07:16:29.252992Z",
     "iopub.status.idle": "2021-08-02T07:21:07.732344Z",
     "shell.execute_reply": "2021-08-02T07:21:07.732925Z",
     "shell.execute_reply.started": "2021-08-02T06:12:51.312624Z"
    },
    "papermill": {
     "duration": 278.510684,
     "end_time": "2021-08-02T07:21:07.733118",
     "exception": false,
     "start_time": "2021-08-02T07:16:29.222434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [01:06<04:27, 66.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [02:00<02:57, 59.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [02:53<01:52, 56.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [03:46<00:55, 55.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n",
      "Model pushed to 1 GPU(s), type Tesla P100-PCIE-16GB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:38<00:00, 55.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# 使用了一个roberta_base和两个robert_large模型\n",
    "pred_df1 = pd.DataFrame()\n",
    "pred_df2 = pd.DataFrame()\n",
    "pred_df3 = pd.DataFrame()\n",
    "\n",
    "for fold in tqdm(range(5)):\n",
    "    pred_df1[f'fold{fold}'] = run(fold%5, '../input/roberta-base/', '../input/commonlit-roberta-base-i/')\n",
    "    pred_df2[f'fold{fold+5}'] = run(fold%5, '../input/robertalarge/', '../input/roberta-large-itptfit/')\n",
    "    pred_df3[f'fold{fold+10}'] = run(fold%5, '../input/robertalarge/', '../input/commonlit-roberta-large-ii/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:21:07.808321Z",
     "iopub.status.busy": "2021-08-02T07:21:07.806850Z",
     "iopub.status.idle": "2021-08-02T07:21:07.809554Z",
     "shell.execute_reply": "2021-08-02T07:21:07.810034Z",
     "shell.execute_reply.started": "2021-08-02T06:17:57.946433Z"
    },
    "papermill": {
     "duration": 0.044834,
     "end_time": "2021-08-02T07:21:07.810192",
     "exception": false,
     "start_time": "2021-08-02T07:21:07.765358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 得到所有模型分别的预测结果\n",
    "pred_df1 = np.array(pred_df1)\n",
    "pred_df2 = np.array(pred_df2)\n",
    "pred_df3 = np.array(pred_df3)\n",
    "# 对不同模型根据成绩和预估可能过拟合的情况分配相应权重\n",
    "model2_predictions = (pred_df2.mean(axis=1) * 0.5) + (pred_df1.mean(axis=1) * 0.3) + (pred_df3.mean(axis=1) * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030884,
     "end_time": "2021-08-02T07:21:07.873030",
     "exception": false,
     "start_time": "2021-08-02T07:21:07.842146",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 3\n",
    "\n",
    "Inspired from: https://www.kaggle.com/ragnar123/commonlit-readability-roberta-tf-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:21:07.945967Z",
     "iopub.status.busy": "2021-08-02T07:21:07.945475Z",
     "iopub.status.idle": "2021-08-02T07:21:07.970520Z",
     "shell.execute_reply": "2021-08-02T07:21:07.970065Z",
     "shell.execute_reply.started": "2021-08-02T06:17:57.955579Z"
    },
    "papermill": {
     "duration": 0.063545,
     "end_time": "2021-08-02T07:21:07.970629",
     "exception": false,
     "start_time": "2021-08-02T07:21:07.907084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import backend as K\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "from kaggle_datasets import KaggleDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:21:08.042047Z",
     "iopub.status.busy": "2021-08-02T07:21:08.041520Z",
     "iopub.status.idle": "2021-08-02T07:21:08.174725Z",
     "shell.execute_reply": "2021-08-02T07:21:08.174204Z",
     "shell.execute_reply.started": "2021-08-02T06:17:58.022184Z"
    },
    "papermill": {
     "duration": 0.172941,
     "end_time": "2021-08-02T07:21:08.174869",
     "exception": false,
     "start_time": "2021-08-02T07:21:08.001928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 配置\n",
    "# 训练折数\n",
    "FOLDS = 5\n",
    "\n",
    "# 模型读取文本的最大长度\n",
    "MAX_LEN = 250\n",
    "\n",
    "# G获取我们想使用的模型\n",
    "MODEL = '../input/tfroberta-base'\n",
    "\n",
    "# 加载模型分词器\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:21:08.255936Z",
     "iopub.status.busy": "2021-08-02T07:21:08.255302Z",
     "iopub.status.idle": "2021-08-02T07:22:20.503766Z",
     "shell.execute_reply": "2021-08-02T07:22:20.502780Z",
     "shell.execute_reply.started": "2021-08-02T06:17:58.140646Z"
    },
    "papermill": {
     "duration": 72.297552,
     "end_time": "2021-08-02T07:22:20.503921",
     "exception": false,
     "start_time": "2021-08-02T07:21:08.206369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicting with model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tfroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicting with model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tfroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicting with model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tfroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicting with model 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tfroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "Predicting with model 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tfroberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# 这个函数根据转换器模型分词器对文本进行分词（分词器tokenizer的作用是将文本转换成序列）\n",
    "def regular_encode(texts, tokenizer, maxlen = MAX_LEN):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = maxlen,\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "\n",
    "# 该函数的作用为编码输入文本\n",
    "def encode_texts(x_test, MAX_LEN):\n",
    "    x_test = regular_encode(x_test.tolist(), tokenizer, maxlen = MAX_LEN)\n",
    "    return x_test\n",
    "\n",
    "# 函数作用为构建我们的模型\n",
    "def build_roberta_base_model(max_len = MAX_LEN):\n",
    "    transformer = TFRobertaModel.from_pretrained(MODEL)\n",
    "    input_word_ids = tf.keras.layers.Input(shape = (max_len, ), dtype = tf.int32, name = 'input_word_ids')\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "    # We only need the cls_token, resulting in a 2d array\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    output = tf.keras.layers.Dense(1, activation = 'linear', dtype = 'float32')(cls_token)\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids], outputs = [output])\n",
    "    return model\n",
    "\n",
    "# 推理函数\n",
    "def roberta_base_inference1():\n",
    "    # 读取测试数据\n",
    "    df = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "    # 获得文本特征\n",
    "    x_test = df['excerpt']\n",
    "    # Roberta 分词器(tokenizer)编码文本\n",
    "    x_test = encode_texts(x_test, MAX_LEN)\n",
    "    # 初始化一个空向量来存储预测\n",
    "    predictions = np.zeros(len(df))\n",
    "    # 5个预测模型\n",
    "    for i in range(FOLDS):\n",
    "        print('\\n')\n",
    "        print('-'*50)\n",
    "        print(f'Predicting with model {i + 1}')\n",
    "        # 构建模型\n",
    "        model = build_roberta_base_model(max_len = MAX_LEN)\n",
    "        # 读取预训练权重\n",
    "        model.load_weights(f'../input/epochs-100-lr-4e5-seed-123/Roberta_Base_123_{i + 1}.h5')\n",
    "        # 预测\n",
    "        fold_predictions = model.predict(x_test).reshape(-1)\n",
    "        # 得到每折预测结果的平均值\n",
    "        predictions += fold_predictions / FOLDS\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "model3_predictions = roberta_base_inference1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039186,
     "end_time": "2021-08-02T07:22:20.578826",
     "exception": false,
     "start_time": "2021-08-02T07:22:20.539640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 4 \n",
    "\n",
    "Inspired from: https://www.kaggle.com/jcesquiveld/best-transformer-representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:22:20.656613Z",
     "iopub.status.busy": "2021-08-02T07:22:20.656036Z",
     "iopub.status.idle": "2021-08-02T07:22:20.723671Z",
     "shell.execute_reply": "2021-08-02T07:22:20.723030Z",
     "shell.execute_reply.started": "2021-08-02T06:19:17.421737Z"
    },
    "papermill": {
     "duration": 0.109009,
     "end_time": "2021-08-02T07:22:20.723785",
     "exception": false,
     "start_time": "2021-08-02T07:22:20.614776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, SequentialSampler, RandomSampler, DataLoader\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:22:20.812463Z",
     "iopub.status.busy": "2021-08-02T07:22:20.811759Z",
     "iopub.status.idle": "2021-08-02T07:22:20.815125Z",
     "shell.execute_reply": "2021-08-02T07:22:20.815509Z",
     "shell.execute_reply.started": "2021-08-02T06:19:17.506401Z"
    },
    "papermill": {
     "duration": 0.052609,
     "end_time": "2021-08-02T07:22:20.815640",
     "exception": false,
     "start_time": "2021-08-02T07:22:20.763031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练输入数据和模型的地址\n",
    "INPUT_DIR = '../input/commonlitreadabilityprize'\n",
    "MODEL_DIR = '../input/roberta-transformers-pytorch/roberta-large'\n",
    "CHECKPOINT_DIR1 = '../input/clrp-mean-pooling/'\n",
    "CHECKPOINT_DIR2 = '../input/clrp-mean-pooling-seeds-17-43/'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "TEST_BATCH_SIZE = 1\n",
    "HIDDEN_SIZE = 1024\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "SEEDS = [113, 71]\n",
    "\n",
    "test = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:22:20.896624Z",
     "iopub.status.busy": "2021-08-02T07:22:20.895875Z",
     "iopub.status.idle": "2021-08-02T07:22:20.899769Z",
     "shell.execute_reply": "2021-08-02T07:22:20.899300Z",
     "shell.execute_reply.started": "2021-08-02T06:19:17.521481Z"
    },
    "papermill": {
     "duration": 0.049373,
     "end_time": "2021-08-02T07:22:20.899890",
     "exception": false,
     "start_time": "2021-08-02T07:22:20.850517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型定义（平均池化模型）\n",
    "class MeanPoolingModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=config)\n",
    "        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \n",
    "        outputs = self.model(input_ids, attention_mask)\n",
    "        last_hidden_state = outputs[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        logits = self.linear(mean_embeddings)\n",
    "        \n",
    "        preds = logits.squeeze(-1).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n",
    "            return loss\n",
    "        else:\n",
    "            return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:22:20.977716Z",
     "iopub.status.busy": "2021-08-02T07:22:20.977122Z",
     "iopub.status.idle": "2021-08-02T07:22:21.200482Z",
     "shell.execute_reply": "2021-08-02T07:22:21.199946Z",
     "shell.execute_reply.started": "2021-08-02T06:19:17.533076Z"
    },
    "papermill": {
     "duration": 0.263538,
     "end_time": "2021-08-02T07:22:21.200615",
     "exception": false,
     "start_time": "2021-08-02T07:22:20.937077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_loader(data):\n",
    "\n",
    "    x_test = data.excerpt.tolist()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "\n",
    "    encoded_test = tokenizer.batch_encode_plus(\n",
    "        x_test, \n",
    "        add_special_tokens=True, \n",
    "        return_attention_mask=True, \n",
    "        padding='max_length', \n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    dataset_test = TensorDataset(\n",
    "        encoded_test['input_ids'],\n",
    "        encoded_test['attention_mask']\n",
    "    )\n",
    "\n",
    "    dataloader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        sampler = SequentialSampler(dataset_test),\n",
    "        batch_size=TEST_BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    return dataloader_test\n",
    "\n",
    "test_dataloader = get_test_loader(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:22:21.305927Z",
     "iopub.status.busy": "2021-08-02T07:22:21.304993Z",
     "iopub.status.idle": "2021-08-02T07:26:22.078294Z",
     "shell.execute_reply": "2021-08-02T07:26:22.077844Z",
     "shell.execute_reply.started": "2021-08-02T06:19:17.753708Z"
    },
    "papermill": {
     "duration": 240.839627,
     "end_time": "2021-08-02T07:26:22.078428",
     "exception": false,
     "start_time": "2021-08-02T07:22:21.238801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ba0ae1536b42038813b379fe005883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_114_1.pth\n",
      "\n",
      "Using model_114_2.pth\n",
      "\n",
      "Using model_114_3.pth\n",
      "\n",
      "Using model_114_4.pth\n",
      "\n",
      "Using model_114_5.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc0e8cbe13349b09d317e939af3db5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using model_72_1.pth\n",
      "\n",
      "Using model_72_2.pth\n",
      "\n",
      "Using model_72_3.pth\n",
      "\n",
      "Using model_72_4.pth\n",
      "\n",
      "Using model_72_5.pth\n"
     ]
    }
   ],
   "source": [
    "# 模型预测结果保存 \n",
    "all_predictions = []\n",
    "for seed in SEEDS:\n",
    "    \n",
    "    fold_predictions = []\n",
    "    \n",
    "    for fold in tqdm(range(NUM_FOLDS)):\n",
    "        model_path = f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "        \n",
    "        print(f\"\\nUsing {model_path}\")\n",
    "        \n",
    "        if seed in [113, 71]:\n",
    "            model_path = CHECKPOINT_DIR1 + f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "        else:\n",
    "            model_path = CHECKPOINT_DIR2 + f\"model_{seed + 1}_{fold + 1}.pth\"\n",
    "            \n",
    "        model = MeanPoolingModel(MODEL_DIR)\n",
    "        model.load_state_dict(torch.load(model_path)) \n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        for batch in test_dataloader:\n",
    "\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'labels':         None,\n",
    "                     }\n",
    "\n",
    "     \n",
    "            preds = model(**inputs).item()\n",
    "            predictions.append(preds)\n",
    "            \n",
    "        del model \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() \n",
    "        \n",
    "        fold_predictions.append(predictions)\n",
    "    all_predictions.append(np.mean(fold_predictions, axis=0).tolist())\n",
    "    \n",
    "model4_predictions = np.mean(all_predictions,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041115,
     "end_time": "2021-08-02T07:26:22.158143",
     "exception": false,
     "start_time": "2021-08-02T07:26:22.117028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:26:22.272198Z",
     "iopub.status.busy": "2021-08-02T07:26:22.255031Z",
     "iopub.status.idle": "2021-08-02T07:30:23.180372Z",
     "shell.execute_reply": "2021-08-02T07:30:23.179850Z",
     "shell.execute_reply.started": "2021-08-02T06:27:19.413251Z"
    },
    "papermill": {
     "duration": 240.981874,
     "end_time": "2021-08-02T07:30:23.180510",
     "exception": false,
     "start_time": "2021-08-02T07:26:22.198636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:22,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:22,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:22,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:22,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [00:22,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 , rmse score: 0.47371087181221916\n",
      "Fold 1 , rmse score: 0.2759399586888456\n",
      "Fold 2 , rmse score: 0.2757889178617948\n",
      "Fold 3 , rmse score: 0.26363757173587454\n",
      "Fold 4 , rmse score: 0.27470609978911387\n",
      "mean rmse 0.3127566839775696\n",
      "Fold 0 , rmse score: 0.24790297319827587\n",
      "Fold 1 , rmse score: 0.5029747594364647\n",
      "Fold 2 , rmse score: 0.23706084685476164\n",
      "Fold 3 , rmse score: 0.23096696063760855\n",
      "Fold 4 , rmse score: 0.2436154995918405\n",
      "mean rmse 0.2925042079437903\n",
      "Fold 0 , rmse score: 0.3776516226841351\n",
      "Fold 1 , rmse score: 0.4078474456611516\n",
      "Fold 2 , rmse score: 0.48660077143868474\n",
      "Fold 3 , rmse score: 0.3653338741725369\n",
      "Fold 4 , rmse score: 0.3915142785400578\n",
      "mean rmse 0.4057895984993133\n",
      "Fold 0 , rmse score: 0.2921394718315368\n",
      "Fold 1 , rmse score: 0.27957332343442026\n",
      "Fold 2 , rmse score: 0.28540387014746454\n",
      "Fold 3 , rmse score: 0.45252763248661115\n",
      "Fold 4 , rmse score: 0.29332989341794413\n",
      "mean rmse 0.3205948382635954\n",
      "Fold 0 , rmse score: 0.39438701377721463\n",
      "Fold 1 , rmse score: 0.4221048664733454\n",
      "Fold 2 , rmse score: 0.38994207650243806\n",
      "Fold 3 , rmse score: 0.39060763869028553\n",
      "Fold 4 , rmse score: 0.5116531299840152\n",
      "mean rmse 0.4217389450854598\n"
     ]
    }
   ],
   "source": [
    "# 导入相关的库文件\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (AutoModel, AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "y_ = Fore.YELLOW\n",
    "r_ = Fore.RED\n",
    "g_ = Fore.GREEN\n",
    "b_ = Fore.BLUE\n",
    "m_ = Fore.MAGENTA\n",
    "c_ = Fore.CYAN\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
    "\n",
    "num_bins = int(np.floor(1 + np.log2(len(train_data))))\n",
    "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
    "\n",
    "target = train_data['target'].to_numpy()\n",
    "bins = train_data.bins.to_numpy()\n",
    "# 计算rmse分数\n",
    "def rmse_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true,y_pred))\n",
    "\n",
    "config = {\n",
    "    'batch_size':128,\n",
    "    'max_len':256,\n",
    "    'nfolds':5,\n",
    "    'seed':42,\n",
    "}\n",
    "# 设立随机种子\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed=config['seed'])\n",
    "# 数据集处理\n",
    "class CLRPDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        self.excerpt = df['excerpt'].to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n",
    "                                max_length=config['max_len'],\n",
    "                                padding='max_length',truncation=True)\n",
    "        return encode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "# 注意力头方法    \n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_targets):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.middle_features = hidden_dim\n",
    "\n",
    "        self.W = nn.Linear(in_features, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "        self.out_features = hidden_dim\n",
    "\n",
    "    def forward(self, features):\n",
    "        att = torch.tanh(self.W(features))\n",
    "\n",
    "        score = self.V(att)\n",
    "\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector\n",
    "# 模型    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained('../input/roberta-base')    \n",
    "        self.head = AttentionHead(768,768,1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.linear = nn.Linear(self.head.out_features,1)\n",
    "\n",
    "    def forward(self,**xb):\n",
    "        x = self.roberta(**xb)[0]\n",
    "        x = self.head(x)\n",
    "        return x    \n",
    "# 获取向量        \n",
    "def get_embeddings(df,path,plot_losses=True, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device} is used\")\n",
    "            \n",
    "    model = Model()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('../input/roberta-base')\n",
    "    \n",
    "    ds = CLRPDataset(df,tokenizer)\n",
    "    dl = DataLoader(ds,\n",
    "                  batch_size = config[\"batch_size\"],\n",
    "                  shuffle=False,\n",
    "                  num_workers = 4,\n",
    "                  pin_memory=True,\n",
    "                  drop_last=False\n",
    "                 )\n",
    "        \n",
    "    embeddings = list()\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in tqdm(enumerate(dl)):\n",
    "            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            embeddings.extend(outputs)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    return np.array(embeddings)     \n",
    "\n",
    "# 获取训练集和测试集的向量(由模型输出得到)\n",
    "train_embeddings1 =  get_embeddings(train_data,'../input/roberta-svm-finetune/model0/model0.bin')\n",
    "test_embeddings1 = get_embeddings(test_data,'../input/roberta-svm-finetune/model0/model0.bin')\n",
    "\n",
    "train_embeddings2 =  get_embeddings(train_data,'../input/roberta-svm-finetune/model1/model1.bin')\n",
    "test_embeddings2 = get_embeddings(test_data,'../input/roberta-svm-finetune/model1/model1.bin')\n",
    "\n",
    "train_embeddings3 =  get_embeddings(train_data,'../input/roberta-svm-finetune/model2/model2.bin')\n",
    "test_embeddings3 = get_embeddings(test_data,'../input/roberta-svm-finetune/model2/model2.bin')\n",
    "\n",
    "train_embeddings4 =  get_embeddings(train_data,'../input/roberta-svm-finetune/model3/model3.bin')\n",
    "test_embeddings4 = get_embeddings(test_data,'../input/roberta-svm-finetune/model3/model3.bin')\n",
    "\n",
    "train_embeddings5 =  get_embeddings(train_data,'../input/roberta-svm-finetune/model4/model4.bin')\n",
    "test_embeddings5 = get_embeddings(test_data,'../input/roberta-svm-finetune/model4/model4.bin')\n",
    "# 获取svm预测分数\n",
    "def get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=10,kernel='rbf'):\n",
    "    scores = list()\n",
    "    preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n",
    "    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n",
    "        model = SVR(C=C,kernel=kernel,gamma='auto')\n",
    "        X_train,y_train = X[train_idx], y[train_idx]\n",
    "        X_valid,y_valid = X[valid_idx], y[valid_idx]\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "        prediction = model.predict(X_valid)\n",
    "        score = rmse_score(prediction,y_valid)\n",
    "        print(f'Fold {k} , rmse score: {score}')\n",
    "        scores.append(score)\n",
    "        preds += model.predict(X_test)\n",
    "        \n",
    "    print(\"mean rmse\",np.mean(scores))\n",
    "    return np.array(preds)/nfolds\n",
    "\n",
    "\n",
    "svm_preds1 = get_preds_svm(train_embeddings1,target,test_embeddings1)\n",
    "svm_preds2 = get_preds_svm(train_embeddings2,target,test_embeddings2)\n",
    "svm_preds3 = get_preds_svm(train_embeddings3,target,test_embeddings3)\n",
    "svm_preds4 = get_preds_svm(train_embeddings4,target,test_embeddings4)\n",
    "svm_preds5 = get_preds_svm(train_embeddings5,target,test_embeddings5)\n",
    "# 释放向量信息，清空显存，只保留预测结果\n",
    "del train_embeddings1, test_embeddings1\n",
    "gc.collect() \n",
    "del train_embeddings2, test_embeddings2\n",
    "gc.collect() \n",
    "del train_embeddings3, test_embeddings3\n",
    "gc.collect() \n",
    "del train_embeddings4, test_embeddings4\n",
    "gc.collect() \n",
    "del train_embeddings5, test_embeddings5\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model5_predictions = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:30:23.540841Z",
     "iopub.status.busy": "2021-08-02T07:30:23.539728Z",
     "iopub.status.idle": "2021-08-02T07:30:23.542848Z",
     "shell.execute_reply": "2021-08-02T07:30:23.542334Z",
     "shell.execute_reply.started": "2021-08-02T06:36:19.211327Z"
    },
    "papermill": {
     "duration": 0.094545,
     "end_time": "2021-08-02T07:30:23.542977",
     "exception": false,
     "start_time": "2021-08-02T07:30:23.448432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:30:23.728020Z",
     "iopub.status.busy": "2021-08-02T07:30:23.727314Z",
     "iopub.status.idle": "2021-08-02T07:30:23.730843Z",
     "shell.execute_reply": "2021-08-02T07:30:23.730446Z"
    },
    "papermill": {
     "duration": 0.099467,
     "end_time": "2021-08-02T07:30:23.730948",
     "exception": false,
     "start_time": "2021-08-02T07:30:23.631481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.41964708, -0.49231511, -0.41502767, -2.49319146, -1.84745904,\n",
       "       -1.2765338 ,  0.14622188])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 平均分配各模型权重\n",
    "predictions = (model1_predictions + model2_predictions + model3_predictions + model4_predictions + model5_predictions) / 5\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:30:23.926824Z",
     "iopub.status.busy": "2021-08-02T07:30:23.925669Z",
     "iopub.status.idle": "2021-08-02T07:30:23.943274Z",
     "shell.execute_reply": "2021-08-02T07:30:23.939837Z"
    },
    "papermill": {
     "duration": 0.116391,
     "end_time": "2021-08-02T07:30:23.943475",
     "exception": false,
     "start_time": "2021-08-02T07:30:23.827084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.419647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.492315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.415028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.493191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.847459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.276534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.146222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.419647\n",
       "1  f0953f0a5 -0.492315\n",
       "2  0df072751 -0.415028\n",
       "3  04caf4e0c -2.493191\n",
       "4  0e63f8bea -1.847459\n",
       "5  12537fe78 -1.276534\n",
       "6  965e592c0  0.146222"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.target = predictions\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-02T07:30:24.279706Z",
     "iopub.status.busy": "2021-08-02T07:30:24.278085Z",
     "iopub.status.idle": "2021-08-02T07:30:24.287345Z",
     "shell.execute_reply": "2021-08-02T07:30:24.288047Z"
    },
    "papermill": {
     "duration": 0.186477,
     "end_time": "2021-08-02T07:30:24.288249",
     "exception": false,
     "start_time": "2021-08-02T07:30:24.101772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将结果输出到提交文件\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 908.926154,
   "end_time": "2021-08-02T07:30:27.687898",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-02T07:15:18.761744",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f4f985d67ec4da69f3546976e4a463f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18ba0ae1536b42038813b379fe005883": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6aae5336410b4445931e33b4878d433e",
        "IPY_MODEL_30e47d3750434b3d9ba5a92f60ca4ba4",
        "IPY_MODEL_3c12531ebfc44b0d8e0a8201a63221eb"
       ],
       "layout": "IPY_MODEL_3806cb58bab04c2f8e9eff2ff9890b91"
      }
     },
     "27695a5a15e74e618179e537bae93378": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "292b5e0884fe416b9a109033916bad74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "30e47d3750434b3d9ba5a92f60ca4ba4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0f4f985d67ec4da69f3546976e4a463f",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9cbd115dc15b4d20b28191e23add7db6",
       "value": 5
      }
     },
     "3542f44d80804dfd8c2c925a6eb39d43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3806cb58bab04c2f8e9eff2ff9890b91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3c12531ebfc44b0d8e0a8201a63221eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_27695a5a15e74e618179e537bae93378",
       "placeholder": "​",
       "style": "IPY_MODEL_f6d843e315354a6281f6cb3e69b54246",
       "value": " 5/5 [02:06&lt;00:00, 23.76s/it]"
      }
     },
     "5ab21323e4a840c798c1ff64c29cde7c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a02bdad949a64dfda2a18fdad24750ba",
       "placeholder": "​",
       "style": "IPY_MODEL_97860dbda22945bda728e4eece315f2b",
       "value": "100%"
      }
     },
     "60b82c57530e4c28a4693773ac0fd030": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6aae5336410b4445931e33b4878d433e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_60b82c57530e4c28a4693773ac0fd030",
       "placeholder": "​",
       "style": "IPY_MODEL_b104490a5dac4836998f12471586c279",
       "value": "100%"
      }
     },
     "87cb72472e3b42749b25e2f275733d13": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "97860dbda22945bda728e4eece315f2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9cbd115dc15b4d20b28191e23add7db6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a02bdad949a64dfda2a18fdad24750ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ac2060f6124f4780a9ed3f9ce2662099": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b104490a5dac4836998f12471586c279": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dbc0e8cbe13349b09d317e939af3db5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5ab21323e4a840c798c1ff64c29cde7c",
        "IPY_MODEL_ef85f32e3f5342198f11c9771beba3da",
        "IPY_MODEL_f64cbf13f9ad410c8d5016fbf78ba5e1"
       ],
       "layout": "IPY_MODEL_ac2060f6124f4780a9ed3f9ce2662099"
      }
     },
     "e82c15a9c4aa4e36add5fa3cf6d7025f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ef85f32e3f5342198f11c9771beba3da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3542f44d80804dfd8c2c925a6eb39d43",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_87cb72472e3b42749b25e2f275733d13",
       "value": 5
      }
     },
     "f64cbf13f9ad410c8d5016fbf78ba5e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_292b5e0884fe416b9a109033916bad74",
       "placeholder": "​",
       "style": "IPY_MODEL_e82c15a9c4aa4e36add5fa3cf6d7025f",
       "value": " 5/5 [01:54&lt;00:00, 22.89s/it]"
      }
     },
     "f6d843e315354a6281f6cb3e69b54246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
